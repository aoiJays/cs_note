

# Langchainå…¥é—¨

[TOC]

>   åšçš„æœ€å¤šçš„äº‹æƒ…å°±æ˜¯è‡ªå·±æŸ¥æ–‡æ¡£ï¼ˆ
>
>   æŠŠæ–‡æ¡£æŸ¥å¥½åŸºæœ¬å°±å­¦å®Œäº†
>
>   -   **APIï¼š**é€šä¹‰åƒé—®

## å‰ç½®å·¥ä½œ
>   [Qwen API æ–‡æ¡£]([å¦‚ä½•ä½¿ç”¨é€šä¹‰åƒé—®API_æ¨¡å‹æœåŠ¡çµç§¯(DashScope)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ (aliyun.com)](https://help.aliyun.com/zh/dashscope/developer-reference/use-qwen-by-api))


å…ˆæ‰¾ä¸ªå…è´¹çš„APIç©ç©ï¼ˆ

æ¥å£ä¾æ—§ä½¿ç”¨openaié‚£ä¸€å¥—å·¥å…·ï¼Œæ¯”è¾ƒå‹å¥½

```python
pip install -U openai # å®‰è£…openai åŒ…å¹¶å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬
```



### API-KEYé…ç½®

ç„¶åæ‹¿ä¸€ä¸ªapi-keyå…ˆ

[å¦‚ä½•è·å–é€šä¹‰åƒé—®APIçš„KEY_æ¨¡å‹æœåŠ¡çµç§¯(DashScope)-é˜¿é‡Œäº‘å¸®åŠ©ä¸­å¿ƒ (aliyun.com)](https://help.aliyun.com/zh/dashscope/developer-reference/acquisition-and-configuration-of-api-key?spm=a2c4g.11186623.0.0.65fe46c1Nhryxe)



æ˜¾ç„¶ä¸å¸Œæœ›æŠŠè‡ªå·±çš„API-Keyæ˜¾å¼åœ°æš´éœ²åœ¨ä»£ç é‡Œé¢

æ‰€ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è¿›è¡Œè®¾ç½®

```bash
# ç”¨ API-KEY ä»£æ›¿ YOUR_DASHSCOPE_API_KEY
echo "export DASHSCOPE_API_KEY='YOUR_DASHSCOPE_API_KEY'" >> ~/.bashrc
source ~/.bashrc
echo $DASHSCOPE_API_KEY # è¾“å‡ºæˆåŠŸåˆ™å®Œæˆè®¾ç½®
```



### APIè°ƒç”¨

```python
import openai
import os

# openai.api_key = os.environ["DASHSCOPE_API_KEY"]
# openai.api_base = "https://dashscope.aliyuncs.com/compatible-mode/v1"

def get_completion(prompt, model = 'qwen1.5-1.8b-chat'):
    
    client = openai.OpenAI(
        api_key=os.environ["DASHSCOPE_API_KEY"], # è°ƒç”¨ç¯å¢ƒå˜é‡
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",  
        # å¡«å†™DashScopeæœåŠ¡çš„base_url
    )
    
    messages= [{'role': 'user', 'content': prompt}]
    
    completion = client.chat.completions.create(
        model = model, messages=messages, temperature=0
    )
    
    return completion
    

get_completion('1+1=?').model_dump_json() # è½¬json
# get_completion('1+1=?').choices[0].message.content ç›´æ¥è·å–å›ç­”

```



å¾—åˆ°çš„completionå¯¹è±¡ï¼š

```json
{
    "id": "chatcmpl-bb28bd68-71b9-9265-9ec7-31f196e28d0c",  // å®Œæˆè¯·æ±‚çš„å”¯ä¸€æ ‡è¯†ç¬¦
    "choices": [  // æ¨¡å‹ç”Ÿæˆçš„é€‰é¡¹åˆ—è¡¨
        {
            "finish_reason": "stop",  // ç”Ÿæˆåœæ­¢çš„åŸå› ï¼ˆä¾‹å¦‚ï¼Œåœæ­¢æ ‡è®°ã€æœ€å¤§tokensç­‰ï¼‰
            "index": 0,  // è¯¥é€‰é¡¹åœ¨ç”Ÿæˆé€‰é¡¹åˆ—è¡¨ä¸­çš„ç´¢å¼•
            "logprobs": null,  // tokensçš„å¯¹æ•°æ¦‚ç‡ï¼ˆnullè¡¨ç¤ºæ²¡æœ‰è¿”å›å¯¹æ•°æ¦‚ç‡ï¼‰
            "message": {
                "content": "1 + 1 equals 2.",  // æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
                "refusal": null,  // æ‹’ç»ç†ç”±ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                "role": "assistant",  // æ¶ˆæ¯çš„è§’è‰²ï¼ˆä¾‹å¦‚ï¼Œç”¨æˆ·æˆ–åŠ©æ‰‹ï¼‰
                "function_call": null,  // å‡½æ•°è°ƒç”¨ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                "tool_calls": null  // å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            }
        }
    ],
    "created": 1723016017,  // åˆ›å»ºæ—¶é—´ï¼ˆUNIXæ—¶é—´æˆ³ï¼‰
    "model": "qwen1.5-1.8b-chat",  // ä½¿ç”¨çš„æ¨¡å‹åç§°
    "object": "chat.completion",  // å¯¹è±¡ç±»å‹ï¼ˆè¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªèŠå¤©å®Œæˆè¯·æ±‚ï¼‰
    "service_tier": null,  // æœåŠ¡ç­‰çº§ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
    "system_fingerprint": null,  // ç³»ç»ŸæŒ‡çº¹ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
    "usage": {
        "completion_tokens": 8,  // å®Œæˆå“åº”ä½¿ç”¨çš„tokensæ•°é‡
        "prompt_tokens": 12,  // æç¤ºä½¿ç”¨çš„tokensæ•°é‡
        "total_tokens": 20  // è¯·æ±‚ä¸­ä½¿ç”¨çš„æ€»tokensæ•°é‡
    }
}

```

### Promt ç®€å•åº”ç”¨

è€ƒè™‘è¿™æ ·ä¸€ä¸ªåœºæ™¯ï¼Œæˆ‘éœ€è¦æŠŠæ”¶åˆ°çš„ä¸€æ®µæ–‡æœ¬ï¼Œæ›´æ¢è¡¨è¾¾è¯­æ°”ï¼š

```python
text = 'æˆ‘å¯¹ä½ çš„äº§å“æˆ–æœåŠ¡æ²¡æœ‰ä»»ä½•å…´è¶£ã€‚è¯·ç«‹å³ä»ä½ çš„åå•ä¸­åˆ é™¤æˆ‘çš„å·ç ï¼Œä¸è¦å†æ‰“æ‰°æˆ‘ã€‚'
```

æˆ‘ä»¬å¸Œæœ›æ›´æ¢æˆä¸€ä¸ªå¹³æ·¡çš„è¯­æ°”

```python
style = 'ç¤¼è²Œä¸”è°ˆåå¾—å½“ã€é¥±è¯»è¯—ä¹¦çš„è¯­æ°”'
```

å¯ä»¥æ„å»ºä¸€ä¸ªpromtæ¨¡æ¿ï¼š
```python
prompt = f"ä½¿ç”¨æŒ‡å®šçš„é£æ ¼ã€è¯­æ°”ï¼Œè½¬æ¢ä»¥ä¸‹æ–‡æœ¬ã€‚\nè¦æ±‚çš„é£æ ¼æ˜¯{style}\næ–‡æœ¬ï¼š```{text}```"
'''
ä½¿ç”¨æŒ‡å®šçš„é£æ ¼ã€è¯­æ°”ï¼Œè½¬æ¢ä»¥ä¸‹æ–‡æœ¬ã€‚
è¦æ±‚çš„é£æ ¼æ˜¯ç¤¼è²Œä¸”è°ˆåå¾—å½“ã€é¥±è¯»è¯—ä¹¦çš„è¯­æ°”
æ–‡æœ¬ï¼š```æˆ‘å¯¹ä½ çš„äº§å“æˆ–æœåŠ¡æ²¡æœ‰ä»»ä½•å…´è¶£ã€‚è¯·ç«‹å³ä»ä½ çš„åå•ä¸­åˆ é™¤æˆ‘çš„å·ç ï¼Œä¸è¦å†æ‰“æ‰°æˆ‘ã€‚```
'''
```

è°ƒç”¨APIå³å¯ï¼š

```python
get_completion(prompt)
'''
'å°Šæ•¬çš„å®¢æˆ·ï¼Œ\n\næˆ‘å¸Œæœ›æ‚¨ä¸€åˆ‡éƒ½å¥½ã€‚é‰´äºæˆ‘ä¸€ç›´ä»¥æ¥å¯¹è´µå…¬å¸çš„äº§å“å’ŒæœåŠ¡æŒæœ‰åŸºæœ¬çš„ä¿¡ä»»ï¼Œä½†æ ¹æ®å½“å‰çš„éœ€æ±‚å’Œå®é™…æƒ…å†µï¼Œæˆ‘å†³å®šä¸å†è€ƒè™‘ä¸æ‚¨çš„è”ç³»ï¼Œå¹¶å°†å…¶ä»æˆ‘ä»¬çš„ç°æœ‰å®¢æˆ·åˆ—è¡¨ä¸­ç§»é™¤ã€‚\n\nåœ¨è¿‡å»çš„æ—¥å­é‡Œï¼Œæˆ‘åœ¨å„ç§åœºåˆéƒ½é˜…è¯»è¿‡å¤§é‡å…³äºè´µå…¬å¸çš„æ–‡ç« å’Œè¯„è®ºï¼Œå¯¹å…¶å“è¶Šçš„äº§å“è´¨é‡ã€ä¸“ä¸šæœåŠ¡ä»¥åŠæ·±å…·å½±å“åŠ›çš„å¸‚åœºåœ°ä½æ·±æ„Ÿæ•¬ä½©ã€‚ç„¶è€Œï¼Œæœ€è¿‘åœ¨å¤„ç†ä¸€äº›ä¸ªäººäº‹åŠ¡æ—¶ï¼Œæˆ‘å¶ç„¶å‘ç°äº†ä¸€äº›ä»¤æˆ‘ä¸æ»¡ä¹‹å¤„ï¼Œå¯¼è‡´æˆ‘éœ€è¦é‡æ–°è¯„ä¼°æˆ‘ä¸è´µå…¬å¸ä¹‹é—´çš„åˆä½œå…³ç³»ã€‚\n\nå¯¹äºè¿™ä¸ªå†³å®šï¼Œæˆ‘æœ‰ä»¥ä¸‹å‡ ä¸ªåŸå› ï¼šé¦–å…ˆï¼Œè¿‘æœŸæˆ‘æ¥åˆ°äº†ä¸€ä¸ªé‡è¦çš„å•†åŠ¡ç”µè¯ä¼šè®®ï¼Œè¿™å¯èƒ½æ„å‘³ç€æˆ‘å°†ä¸å¾—ä¸åœ¨éå¸¸å¿™ç¢Œçš„ç¯å¢ƒä¸­å‚ä¸ã€‚åŸºäºè¿™ç§æ—¶é—´å‹åŠ›å’Œä¸ªäººä¼˜å…ˆäº‹é¡¹è€ƒé‡ï¼Œæˆ‘è§‰å¾—æ›´é«˜æ•ˆçš„æ–¹å¼æ˜¯æˆ‘èƒ½ä¸“æ³¨äºè¿™æ¬¡ä¼šè®®ï¼Œè€Œéæ— è°“åœ°è¢«æ‚¨çš„ç”µè¯æ‰“æ–­ã€‚\n\nå…¶æ¬¡ï¼Œæˆ‘å‘ç°è´µå…¬å¸åœ¨æŸäº›æ–¹é¢å­˜åœ¨ä¸€äº›ç–‘è™‘å’Œæœªè§£å†³çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¯èƒ½ä¼šå½±å“åˆ°æˆ‘å¯¹è´µå…¬å¸çš„æ•´ä½“ä¿¡ä»»åº¦å’Œæ»¡æ„åº¦ã€‚é€šè¿‡æŸ¥é˜…ç›¸å…³èµ„æ–™å’Œä¸ç›¸å…³äººå‘˜è¿›è¡Œæ·±å…¥äº¤æµï¼Œæˆ‘äº†è§£åˆ°éƒ¨åˆ†æœåŠ¡é¡¹ç›®å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œä¾‹å¦‚å¤„ç†æ•ˆç‡ä½ä¸‹ã€å”®åæœåŠ¡æœ‰å¾…æå‡ç­‰ã€‚è¿™äº›å®é™…çš„é—®é¢˜ä¸æˆ‘å¯¹è´µå…¬å¸ä»·å€¼çš„è®¤çŸ¥ç›¸å†²çªï¼Œä»è€Œè®©æˆ‘æ„Ÿåˆ°ä¸é€‚å¹¶éš¾ä»¥æ¥å—ã€‚\n\næœ€åï¼Œæˆ‘æ³¨æ„åˆ°æ‚¨çš„æœåŠ¡çƒ­çº¿ç»å¸¸å‡ºç°å™ªéŸ³å¹²æ‰°å’Œé«˜åˆ†è´å™ªå£°ï¼Œä¸¥é‡å½±å“äº†æˆ‘åœ¨é€šè¯è¿‡ç¨‹ä¸­çš„ä¸“æ³¨åŠ›å’Œå·¥ä½œæ•ˆç‡ã€‚è™½ç„¶æˆ‘çŸ¥é“è´µå…¬å¸æ­£åœ¨ç§¯ææ”¹å–„æœåŠ¡è´¨é‡ï¼Œä½†æˆ‘è®¤ä¸ºé€šè¿‡ä¼˜åŒ–ç”µè¯ç³»ç»Ÿçš„è®¾ç½®å’Œç»´æŠ¤ï¼Œå¯ä»¥æ˜¾è‘—æé«˜é€šè¯ä½“éªŒå’Œè´¨é‡ï¼Œè¿›ä¸€æ­¥å¢å¼ºæˆ‘å¯¹è´µå…¬å¸çš„å¿ è¯šåº¦å’Œæ¨èåº¦ã€‚\n\nç»¼ä¸Šæ‰€è¿°ï¼ŒåŸºäºä»¥ä¸Šå‡ ç‚¹å› ç´ ï¼Œæˆ‘å†³å®šåœæ­¢ä¸è´µå…¬å¸çš„ç°æœ‰ä¸šåŠ¡å…³ç³»ï¼Œå¹¶æå‡ºæ­¤è¯·æ±‚ã€‚å¸Œæœ›æ‚¨èƒ½ç†è§£æˆ‘çš„ç«‹åœºï¼Œå¹¶å°½å¿«é‡‡å–è¡ŒåŠ¨ï¼Œä»¥æ»¡è¶³æˆ‘æ‰€æå‡ºçš„æ”¹è¿›éœ€æ±‚å’ŒæœŸæœ›ï¼Œç¡®ä¿æˆ‘ä»¬ä¿æŒè‰¯å¥½çš„æ²Ÿé€šå’Œåˆä½œå…³ç³»ã€‚æ„Ÿè°¢æ‚¨çš„ç†è§£å’Œé…åˆï¼ŒæœŸå¾…åœ¨æœªæ¥æœ‰æœºä¼šä¸è´µå…¬å¸è¿›è¡Œæ›´æ·±å…¥çš„æ¢è®¨å’Œåˆä½œæœºä¼šã€‚\n\né¡ºç¥å•†ç¥ºï¼Œ\n[æ‚¨çš„å§“å]\n[æ‚¨çš„èŒä½]\n[è”ç³»æ–¹å¼]'
'''
```



æ¥ä¸‹æ¥æˆ‘ä»¬å¼€å§‹è®°å½•ï¼Œå¦‚ä½•ä½¿ç”¨Langchainï¼Œå®Œæˆæ›´åŠ å¤æ‚çš„ä»»åŠ¡



## ç»„ä»¶

-   Models
-   Prompts
-   Memory
-   Indexes
-   Chains
-   Agents



## Models

-   LLMs
    -   è¾“å…¥ï¼šæ–‡æœ¬å­—ç¬¦ä¸²
    -   è¾“å‡ºï¼šæ–‡æœ¬å­—ç¬¦ä¸²
-   Chat Models
    -   è¾“å…¥ï¼šèŠå¤©æ¶ˆæ¯åˆ—è¡¨
    -   è¾“å‡ºï¼šèŠå¤©æ¶ˆæ¯
-   Text Embbeding Models
    -   è¾“å…¥ï¼šæ–‡æœ¬
    -   è¾“å‡ºï¼šæµ®ç‚¹æ•°åˆ—è¡¨

### LLMs

>   [æ–‡æ¡£ï¼šModels - LLMs - Tongyi Qwen | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/v0.2/docs/integrations/llms/tongyi/)
>
>   [Tongyiå¯¹è±¡æ–‡æ¡£ï¼šlangchain_community.llms.tongyi.Tongyi â€” ğŸ¦œğŸ”— LangChain 0.2.12](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.tongyi.Tongyi.html)

ä½¿ç”¨çš„æ˜¯é€šä¹‰åƒé—®æä¾›çš„APIï¼Œå› æ­¤å‰å¾€langchainå¯¹äºé€šä¹‰åƒé—®çš„å°è£…æ–‡æ¡£å³å¯



é¦–å…ˆå®‰è£…langchainçš„langchain-communityåº“

å› ä¸ºæ˜¯é€šä¹‰åƒé—®çš„ï¼Œæ‰€ä»¥è¿˜éœ€è¦å®‰è£…ä¸€ä¸‹`dashscope`

```bash
pip install --upgrade --quiet  langchain-community dashscope
```





```python
from langchain_community.llms import Tongyi
llm = Tongyi( 
    model_name="qwen1.5-1.8b-chat", 
    temperature=0.95, 
    top_p=0.7, 
    max_tokens=100 
)
print( llm.invoke("ä½ å¥½å—ï¼Ÿ") ) # å­—ç¬¦ä¸²å½¢å¼è¾“å…¥è¾“å‡º
```

-   `temperature` å‚æ•°ï¼ˆTï¼‰è°ƒæ•´softmaxçš„åˆ†å¸ƒï¼Œæ§åˆ¶æ¨¡å‹è¾“å‡ºçš„éšæœºæ€§

    -   $$
        p_i = \frac{\exp(o_i/T)}{\sum_j \exp{(o_j/T)}}
        $$

    -   Tè¶Šå°ï¼Œç”Ÿæˆè¶Šä¿å®ˆã€‚Tè¶Šå¤§ï¼Œç”Ÿæˆè¶Šéšæœºï¼ˆç›¸å½“äºæ‰€æœ‰æ¦‚ç‡éƒ½ç›¸ç­‰ï¼‰

    -   T=1ï¼Œæ— è°ƒæ•´

-   `top_p`

    -   ç´¯è®¡æ¦‚ç‡é å‰çš„å‡ ä¸ªtokençš„æ¦‚ç‡å€¼ï¼Œå½“æ¦‚ç‡å’Œåˆ°è¾¾`top_p`æ—¶ï¼Œä¸å†è€ƒè™‘å‰©ä¸‹çš„token

-   `top_k`

    -   è€ƒè™‘çš„tokenæ•°é‡

#### æµå¼è¾“å‡º

```python
import asyncio
# æµå¼è¾“å‡º
async def stream_words(prompt, llm):
    async for chunk in llm.astream([prompt]):
        words = chunk.split()
        for word in words:
            print(f"{word}", end='', flush=True) 
   

# è¿è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    prompt = "è®²ä¸€ä¸ªæ•…äº‹ï¼Œä¸è¶…è¿‡30å­—"
    llm = Tongyi( model_name="qwen1.5-1.8b-chat", temperature=0.95, top_p=0.7, max_tokens=10, streaming = True)
    asyncio.run(stream_words(prompt, llm))
```



### Chat Models

>   [æ–‡æ¡£ï¼šModels - Chat Models - ChatTongyi | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/v0.2/docs/integrations/chat/tongyi/)
>
>   [å¯¹è±¡æ–‡æ¡£ï¼šlangchain_community.chat_models.tongyi.ChatTongyi â€” ğŸ¦œğŸ”— LangChain 0.2.12](https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.tongyi.ChatTongyi.html)

```python
from langchain_community.chat_models.tongyi import ChatTongyi
chatLLM = ChatTongyi(
    model_name="qwen1.5-1.8b-chat", temperature=0.95, 
    top_p=0.7, max_tokens=500 
)
```

æˆ‘ä»¬æœ‰ä¸¤ç§æ–¹æ³•å¯¹æ¶ˆæ¯è¿›è¡Œå®šä¹‰

```python
from langchain_core.messages import HumanMessage, SystemMessage
messages = [
    SystemMessage(
        content="You are a helpful assistant that translates English to French."
    ),
    HumanMessage(
        content="Translate this sentence from English to French. I love programming."
    ),
]
```

æˆ–è€…

```python
messages = [
    ("system", "ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç¿»è¯‘å®¶ï¼Œå¯ä»¥å°†ç”¨æˆ·çš„ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚"),
    ("human", "æˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚"),
]
```



æœ€åï¼š

```python
chatLLM.invoke(messages) # æˆ–è€…ç›´æ¥ chatLLM(messages)
# æ¨èinvoke
```

è¿”å›å†…å®¹ï¼š

```python
AIMessage(
    content="J'aime programmer.", 
    response_metadata={
        'model_name': 'qwen1.5-1.8b-chat', 
        'finish_reason': 'stop', 
        'request_id': '810ae9a8-1365-913b-a2d2-f4b6be06cf58', 
        'token_usage': {
            'input_tokens': 36, 
            'output_tokens': 5, 
            'total_tokens': 41
        }
    }, 
    id='run-5ee38ee1-f1f2-4b65-9b37-584dd64ee2ee-0'
)
```

ç›¸æ¯”ä½¿ç”¨LLMsè·å¾—äº†æ›´å¤šå…¶ä»–ä¿¡æ¯



### Embedding models

>   [æ–‡æ¡£ï¼šDashScope | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/v0.2/docs/integrations/text_embedding/dashscope/)

```python
from langchain_community.embeddings import DashScopeEmbeddings
import os

embeddings = DashScopeEmbeddings(
    model="text-embedding-v1", dashscope_api_key=os.environ["DASHSCOPE_API_KEY"]
)

text = "Kurokawa Akane"

# å•ä¸ªæ–‡æœ¬çš„è¯åµŒå…¥
query_result = embeddings.embed_query(text)
print(query_result)

# å¤šä¸ª
doc_results = embeddings.embed_documents(["Kurokawa Akane","Kurokawa Akane"])
print(doc_results)

# å‡è¾“å‡ºæµ®ç‚¹æ•°åˆ—è¡¨
```



## Prompts

### PromptTemplate

> [æ–‡æ¡£ï¼šPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

```python
from langchain_community.llms import Tongyi
from langchain_core.prompts import PromptTemplate

# æ–¹æ³•1ï¼šPromptTemplateæ„é€ å‡½æ•°

# æ¨¡æ¿å®šä¹‰
template = "ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹æ˜æ—¥æ–¹èˆŸå¹²å‘˜{Operators}çš„èº«ä»½"
# æ„å»ºç»„ä»¶
prompt = PromptTemplate(template=template, input_variables=["Operators"])
# ç”Ÿæˆæ¨¡æ¿
prompt_text = prompt.format(Operators="ç¼ªå°”èµ›æ–¯")

# --------------------------------------

# æ–¹æ³•2ï¼šfrom_template
template = "ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹æ˜æ—¥æ–¹èˆŸå¹²å‘˜{Operators}çš„èº«ä»½"
prompt = PromptTemplate.from_template(template)
prompt_text = prompt.format(Operators="ç¼ªå°”èµ›æ–¯")

# --------------------------------------



llm = Tongyi( model_name="qwen1.5-1.8b-chat", temperature=0.95, top_p=0.7, max_tokens=100 )

llm(prompt_text)
```

### FewShotPromptTemplate

> [æ–‡æ¡£ï¼šFewShotPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html#)
>
> - Zero-shot æ˜¯æŒ‡æ¨¡å‹åœ¨æ²¡æœ‰è§è¿‡ä»»ä½•ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œé¢„æµ‹
> - One-shot æ˜¯æŒ‡æ¨¡å‹åœ¨ä»…è§è¿‡ä¸€ä¸ªè®­ç»ƒæ ·æœ¬çš„æƒ…å†µä¸‹è¿›è¡Œå­¦ä¹ å’Œé¢„æµ‹
> - Few-shot æ˜¯æŒ‡æ¨¡å‹åœ¨è§è¿‡å¾ˆå°‘é‡çš„è®­ç»ƒæ ·æœ¬ï¼ˆå‡ ä¸ªåˆ°å‡ åä¸ªï¼‰çš„æƒ…å†µä¸‹è¿›è¡Œå­¦ä¹ å’Œé¢„æµ‹

```python
from langchain_core.prompts.few_shot import FewShotPromptTemplate


# æ ·ä¾‹å‡†å¤‡
# ç›¸å½“äºä¼šç»„åˆæ ·ä¾‹ä»¥åŠæ¨¡æ¿ æˆä¸ºçœŸæ­£çš„æ ·ä¾‹
examples = [
    {
        "Operators": "ä»¤", 
        "description": "ä»¤ï¼Œä¸€ä½è¾…åŠ©å¹²å‘˜"
    },
    {
        "Operators": "æ–¯å¡è’‚", 
        "description": "æ–¯å¡è’‚ï¼Œä¸€ä½è¿‘å«å¹²å‘˜"
    }, 
    {
        "Operators": "ç¼ªå°”èµ›æ–¯", 
        "description": "ç¼ªå°”èµ›æ–¯, ä¸€ä½å…ˆé”‹å¹²å‘˜"
    }
]
example_template = """
æ˜æ—¥æ–¹èˆŸå¹²å‘˜ï¼š{Operators}
æè¿°ï¼š{description}\n
"""
example_prompt = PromptTemplate.from_template(example_template)


few_shot_prompt = FewShotPromptTemplate(
    examples = examples, 
    example_prompt = example_prompt, 
    
	# çœŸå®æé—®
    prefix = "ç»™å‡ºæ¯ä¸ªå¹²å‘˜çš„å§“åï¼Œæè¿°æ¯ä¸€ä½å¹²å‘˜çš„èº«ä»½",
    suffix = "æ˜æ—¥æ–¹èˆŸå¹²å‘˜ï¼š{Operators}\næè¿°ï¼š\n",
    input_variables=["Operators"],
    example_separator = "\n" # æ ·ä¾‹ç›´æ¥éš”å¼€
)

prompt_text = few_shot_prompt.format(Operators="é»")
print(prompt_text)

'''
# prompt_text:
ç»™å‡ºæ¯ä¸ªå¹²å‘˜çš„å§“åï¼Œæè¿°æ¯ä¸€ä½å¹²å‘˜çš„èº«ä»½

æ˜æ—¥æ–¹èˆŸå¹²å‘˜ï¼šä»¤
æè¿°ï¼šä»¤ï¼Œä¸€ä½è¾…åŠ©å¹²å‘˜



æ˜æ—¥æ–¹èˆŸå¹²å‘˜ï¼šæ–¯å¡è’‚
æè¿°ï¼šæ–¯å¡è’‚ï¼Œä¸€ä½è¿‘å«å¹²å‘˜



æ˜æ—¥æ–¹èˆŸå¹²å‘˜ï¼šç¼ªå°”èµ›æ–¯
æè¿°ï¼šç¼ªå°”èµ›æ–¯, ä¸€ä½å…ˆé”‹å¹²å‘˜


æ˜æ—¥æ–¹èˆŸå¹²å‘˜ï¼šé»
æè¿°ï¼š
'''
print(llm(prompt_text))
'''
# è¾“å‡º
æ˜æ—¥æ–¹èˆŸå¹²å‘˜ï¼šè°·ç²’
æè¿°ï¼šè°·ç²’ï¼Œä¸€ä½ç ”ç©¶è€…å¹²å‘˜

# è‡³å°‘æ ¼å¼å¯¹äº†
'''
```



## Chains

### LLMChain

> [å¯¹è±¡æ–‡æ¡£ï¼šLLMChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html#langchain.chains.llm.LLMChain)

```python
from langchain.chains import LLMChain
# æŒ‡å®šæ¨¡å‹ æŒ‡å®šprompt
chain1 = LLMChain(llm=llm, prompt=few_shot_prompt)
chain1.invoke("é»")
# {'Operators': 'é»', 'text': 'æ˜æ—¥æ–¹èˆŸå¹²å‘˜ï¼šè°·ç²’\næè¿°ï¼šè°·ç²’ï¼Œä¸€ä½ç ”ç©¶è€…å¹²å‘˜'}
```

éœ€è¦å¤šä¸ªå‚æ•°æ—¶ï¼š

```python
# æ¨¡æ¿å®šä¹‰
template = "{a} + {b} = ?"
# æ„å»ºç»„ä»¶
prompt = PromptTemplate.from_template(template)

chain2 = LLMChain(llm=llm, prompt=prompt)
chain2.invoke({"a": "100", "b": "200"})
# {'a': '100', 'b': '200', 'text': '300'}
```

### SimpleSequentialChain

å°†ä¸Šä¸€ä¸ªchainçš„è¾“å‡ºä½œä¸ºå½“å‰chainçš„è¾“å…¥

```python
template = "{country}çš„é¦–éƒ½æ˜¯å“ªä¸ªåŸå¸‚ï¼Ÿç”¨ä¸€ä¸ªè¯å›ç­”"
prompt = PromptTemplate.from_template(template)
chain1 = LLMChain(llm=llm, prompt=prompt)

template2 = "{city}æœ‰å“ªäº›æ™¯ç‚¹å€¼å¾—å‚è§‚ï¼Ÿç”¨ä¸€å¥è¯åˆ—ä¸¾"
prompt2 = PromptTemplate.from_template(template2)
chain2 = LLMChain(llm=llm, prompt=prompt2)

# ç»„è£…
from langchain.chains import SimpleSequentialChain
chains = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)
# verbose=Trueï¼Œå¯ä»¥çœ‹è§æ¨ç†çš„æ­¥éª¤
chains.run("ä¸­å›½")

'''
> Entering new SimpleSequentialChain chain...
é¦–éƒ½ï¼šåŒ—äº¬ã€‚
æ•…å®«ã€å¤©å®‰é—¨å¹¿åœºã€é•¿åŸã€é¢å’Œå›­ã€é¸Ÿå·¢ç­‰ã€‚
> Finished chain.
'''

'''
# çœŸå®è¾“å‡º
'æ•…å®«ã€å¤©å®‰é—¨å¹¿åœºã€é•¿åŸã€é¢å’Œå›­ã€é¸Ÿå·¢ç­‰ã€‚'
'''

```

### LLMChainï¼ˆå†è§£é‡Šï¼‰

æ¯ä¸€ä¸ªLLMChainï¼Œç»‘å®šäº†æ¨¡å‹å’Œprompt

æˆä¸ºä¸€ä¸ªå•ç‹¬çš„å…ƒç´ 



```python
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate

# LLMChain1 è‹±æ–‡ç¿»è¯‘ä¸­æ–‡
chain1_prompt = PromptTemplate.from_template('''
è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆæµç•…çš„ä¸­æ–‡ï¼š
æ–‡æœ¬ï¼š{txt1}
ç¿»è¯‘ï¼š
''')
chain1 = LLMChain(llm=llm, prompt=chain1_prompt)

# LLMChain2 ä¸­æ–‡å½’çº³
chain2_prompt = PromptTemplate.from_template('''
è¯·ç”¨ç®€æ´çš„ä¸€å¥è¯ï¼Œæ¦‚æ‹¬ä»¥ä¸‹æ–‡æœ¬ï¼š
æ–‡æœ¬ï¼š{txt2}
æ¦‚æ‹¬ï¼š
''')
chain2 = LLMChain(llm=llm, prompt=chain2_prompt)

# LLMChain3 æ–°é—»æ ‡é¢˜
chain3_prompt = PromptTemplate.from_template('''
è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´ä¸”å¸å¼•äººçš„æ–°é—»æ ‡é¢˜ï¼š
æ–‡æœ¬ï¼š{txt3}
æ–°é—»æ ‡é¢˜ï¼š
''')
chain3 = LLMChain(llm=llm, prompt=chain3_prompt)
```



### SimpleSequentialChainï¼ˆå†è§£é‡Šï¼‰

- æ¯ä¸ªLLMChainçš„è¾“å‡ºï¼Œå°†ä½œä¸ºä¸‹ä¸€ä¸ªLLMChainçš„è¾“å…¥

```python
from langchain.chains import SimpleSequentialChain
chains = SimpleSequentialChain(
    chains = [ chain1, chain2, chain3], 
    verbose = True
)

chains.invoke(txt)
```

è¿è¡Œç»“æœï¼š

```python
'''
> Entering new SimpleSequentialChain chain...
è¿‘å¹´æ¥ï¼Œè«å°”æ–¯è¥¿å°å§åœ¨å“¥ä¼¦æ¯”äºšå­¦æœ¯ç•Œäº«æœ‰æé«˜çš„å£°èª‰ã€‚å¥¹åœ¨ä¸åˆ°30å²çš„æ—¶å€™ï¼Œå°±å·²ç»æˆä¸ºRhein Lifeåå¼ºäººç‰©ä¹‹ä¸€ï¼Œæ‹…ä»»ç”Ÿæ€å­¦ä¸ç¯å¢ƒç§‘å­¦å¤§å­¦å‰¯æ ¡é•¿ä¸€èŒï¼Œå¹¶æ‹…ä»»ç‰¹é›·è’™å¤šå¤§å­¦å®¢åº§è®²å¸ˆã€‚è¿‡å»åå¹´é—´ï¼Œè«å°”æ–¯è¥¿åŠå…¶ç ”ç©¶å›¢é˜Ÿåœ¨ç”Ÿæ€å­¦ã€é—ä¼ å­¦ã€æ¤ç‰©ç”Ÿç‰©åŒ–å­¦å’Œæ¤ç‰©ç”Ÿç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çªç ´ï¼Œå¯¹å“¥ä¼¦æ¯”äºšç”Ÿå‘½ç§‘å­¦å’Œç¯å¢ƒç§‘å­¦çš„å‘å±•åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚
è«å°”æ–¯è¥¿å°å§åœ¨å“¥ä¼¦æ¯”äºšå­¦æœ¯ç•Œäº«æœ‰æé«˜å£°èª‰ï¼Œæ˜¯çŸ¥åç”Ÿæ€å­¦ä¸ç¯å¢ƒç§‘å­¦å¤§å­¦å‰¯æ ¡é•¿åŠç‰¹é›·è’™å¤šå¤§å­¦å®¢åº§è®²å¸ˆï¼Œåœ¨è¿‘åå¹´é—´åœ¨å¤šä¸ªé¢†åŸŸå–å¾—æ˜¾è‘—çªç ´ï¼Œå¯¹å“¥ä¼¦æ¯”äºšç”Ÿå‘½ç§‘å­¦å’Œç¯å¢ƒç§‘å­¦å‘å±•åšå‡ºé‡è¦è´¡çŒ®ã€‚
"å“¥ä¼¦æ¯”äºšå­¦è€…è«å°”æ–¯è¥¿å°å§ï¼šæ°å‡ºç”Ÿæ€å­¦å®¶ã€å®¢åº§è®²å¸ˆï¼Œè¿‘åå¹´é‡Œå¤šé¡¹ç ”ç©¶æˆæœç©ç›®ï¼Œå¼•é¢†å“¥ä¼¦æ¯”äºšç”Ÿå‘½ç§‘å­¦å’Œç¯å¢ƒç§‘å­¦å‘å±•"

> Finished chain.
'''
```



### SequentialChain

æˆ‘ä»¬éœ€è¦æ›´åŠ å¤æ‚çš„chainå…³ç³»

- ç¬¬ä¸‰ä¸ªLLMChainçš„è¾“å…¥ï¼Œç”±ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªæä¾›â€¦â€¦



æˆ‘ä»¬å°†å¯¹æ¨¡æ¿ä»¥åŠLLMChainçš„keyéå¸¸æ•æ„Ÿ



```python
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate

# LLMChain1 è‹±æ–‡ç¿»è¯‘ä¸­æ–‡
chain1_prompt = PromptTemplate.from_template('''
è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆæµç•…çš„ä¸­æ–‡ï¼š
æ–‡æœ¬ï¼š{Source_txt}
ç¿»è¯‘ï¼š
''')
chain1 = LLMChain(llm=llm, prompt=chain1_prompt, output_key='Translated_txt')


# LLMChain2 è¯­è¨€åˆ†ç±»
chain2_prompt = PromptTemplate.from_template('''
è¿™ä¸€æ®µæ–‡æœ¬ä½¿ç”¨çš„æ˜¯å“ªä¸€ç§è¯­è¨€ï¼Ÿ
æ–‡æœ¬ï¼š{Source_txt}
è¯­è¨€ï¼š
''')
chain2 = LLMChain(llm=llm, prompt=chain2_prompt, output_key='answer')

# LLMChain3 ä¸­æ–‡å½’çº³
chain3_prompt = PromptTemplate.from_template('''
è¯·ç”¨ç®€æ´çš„ä¸€å¥è¯ï¼Œæ¦‚æ‹¬ä»¥ä¸‹æ–‡æœ¬ï¼š
æ–‡æœ¬ï¼š{Translated_txt}
æ¦‚æ‹¬ï¼š
''')
chain3 = LLMChain(llm=llm, prompt=chain3_prompt, output_key='summary')

```



```python
from langchain.chains import SequentialChain
chains = SequentialChain(
    chains = [chain1, chain2, chain3],
    input_variables = ['Source_txt'],
    output_variables = ['Translated_txt','answer', 'summary'], # å¦‚æœå¸Œæœ›è¿”å›ä¸­é—´çš„è¾“å‡ºç»“æœ
    verbose=True
)

chains.invoke(txt)
'''
{'Source_txt': 'Miss Muelsyse is a highly regarded genius scientist in the academic circles of Columbia in recent years. At less than thirty years old, she has already become one of the top ten figures at Rhine Life, serving as the Director of Ecology and a guest lecturer at the University of Trimondo. Over the past decade, Miss Muelsyse and her research team have made significant advancements in various fields, including ecology, genetics, plant biochemistry, and plant physiology, greatly contributing to the development of life sciences and environmental sciences in Columbia.',
 'Translated_txt': 'è¿‘å¹´æ¥ï¼Œè«å°”æ–¯è¥¿å°å§åœ¨å“¥ä¼¦æ¯”äºšå­¦æœ¯ç•Œäº«æœ‰æé«˜çš„å£°èª‰ã€‚å¥¹åœ¨ä¸åˆ°30å²çš„æ—¶å€™ï¼Œå°±å·²ç»æˆä¸ºRhein Lifeåå¼ºäººç‰©ä¹‹ä¸€ï¼Œæ‹…ä»»ç”Ÿæ€å­¦ä¸ç¯å¢ƒç§‘å­¦å¤§å­¦å‰¯æ ¡é•¿ä¸€èŒï¼Œå¹¶æ‹…ä»»ç‰¹é›·è’™å¤šå¤§å­¦å®¢åº§è®²å¸ˆã€‚è¿‡å»åå¹´é—´ï¼Œè«å°”æ–¯è¥¿åŠå…¶ç ”ç©¶å›¢é˜Ÿåœ¨ç”Ÿæ€å­¦ã€é—ä¼ å­¦ã€æ¤ç‰©ç”Ÿç‰©åŒ–å­¦å’Œæ¤ç‰©ç”Ÿç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çªç ´ï¼Œå¯¹å“¥ä¼¦æ¯”äºšç”Ÿå‘½ç§‘å­¦å’Œç¯å¢ƒç§‘å­¦çš„å‘å±•åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚',
 'answer': 'è¿™æ®µæ–‡æœ¬ä½¿ç”¨äº†è‹±è¯­ã€‚',
 'summary': 'è«å°”æ–¯è¥¿å°å§åœ¨å“¥ä¼¦æ¯”äºšå­¦æœ¯ç•Œäº«æœ‰æé«˜å£°èª‰ï¼Œæ˜¯çŸ¥åç”Ÿæ€å­¦ä¸ç¯å¢ƒç§‘å­¦å¤§å­¦å‰¯æ ¡é•¿åŠç‰¹é›·è’™å¤šå¤§å­¦å®¢åº§è®²å¸ˆï¼Œåœ¨è¿‘åå¹´é—´åœ¨å¤šä¸ªé¢†åŸŸå–å¾—æ˜¾è‘—çªç ´ï¼Œå¯¹å“¥ä¼¦æ¯”äºšç”Ÿå‘½ç§‘å­¦å’Œç¯å¢ƒç§‘å­¦å‘å±•åšå‡ºé‡è¦è´¡çŒ®ã€‚'}
'''
```

## Agents

é¢„è®­ç»ƒçš„LLMèƒ½åŠ›æœ‰é™ï¼ˆæ•°å­¦èƒ½åŠ›ã€é€»è¾‘ã€å®æ—¶ä¿¡æ¯ï¼‰ï¼Œéœ€è¦å¼•å…¥ç¬¬ä¸‰æ–¹å·¥å…·è¿›è¡Œè¾…åŠ©

>   [Toolkits | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/v0.2/docs/integrations/toolkits/)
>
>   [Tools | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/v0.2/docs/integrations/tools/)

```python
!pip install numexpr
from langchain.agents import load_tools, initialize_agent, AgentType

# åŠ è½½ç¬¬ä¸‰æ–¹å·¥å…·
tools = load_tools(["llm-math"], llm=llm)
# åˆå§‹åŒ–agent
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

agent.run("Assume you are 12 years old now, what's your age raised to the 0.43 power?")

'''
To calculate the amount by which my current age is raised to the power of 0.43, we first need to determine my current age and then multiply it by 0.43.
My current age:
Since I am currently 12 years old, my current age is 12.
To find the power 0.43, we divide the current age by 0.43:
Power = Current Age / 0.43
Power = 12 / 0.43
Power â‰ˆ 30.867294153885714
Thought: The calculation has been completed successfully.
Final Answer: My current age raised to the 0.43 power is approximately 30.87.

> Finished chain.
'''
```



## Memory

### ChatMessageHistory

-   è®°å½•æ¶ˆæ¯å­˜å‚¨è®°å½•

```python
from langchain.memory import ChatMessageHistory

# è®°å½•æ¶ˆæ¯å­˜å‚¨è®°å½•
history = ChatMessageHistory()

history.add_user_message("ä»Šå¤©å¼€å­¦") # user
history.add_ai_message("å¥½çš„ï¼Œæ˜å¤©è§") # ai

history.messages # æŸ¥çœ‹æ¶ˆæ¯è®°å½•

# [HumanMessage(content='ä»Šå¤©å¼€å­¦'), AIMessage(content='å¥½çš„ï¼Œæ˜å¤©è§')]
```



### é•¿æœŸè®°å¿†

æˆ‘ä»¬éœ€è¦æŠŠä¹‹å‰çš„å¯¹è¯å­˜å‚¨ä¸‹æ¥ï¼Œå¯ä»¥è€ƒè™‘å­˜å‚¨æˆå­—å…¸å½¢å¼

```python
from langchain.memory import ChatMessageHistory
from langchain.schema import messages_to_dict, messages_from_dict

# æ³¨æ„messageså’Œmessageæ˜¯ä¸ä¸€æ ·çš„ï¼Œå‰è€…æ˜¯åˆ—è¡¨ï¼Œåè€…æ˜¯å•ä¸ªæ¶ˆæ¯
# è¿™é‡Œä½¿ç”¨s


history = ChatMessageHistory()

history.add_user_message("ä»Šå¤©å¼€å­¦") # user
history.add_ai_message("å¥½çš„ï¼Œæ˜å¤©è§") # ai

dicts = messages_to_dict(history.messages)
print(dicts)
'''
[{'type': 'human', 'data': {'content': 'ä»Šå¤©å¼€å­¦', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': None, 'example': False}}, {'type': 'ai', 'data': {'content': 'å¥½çš„ï¼Œæ˜å¤©è§', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': None, 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}}]

'''
new_history = messages_from_dict(dicts)
print(new_history)
'''
[HumanMessage(content='ä»Šå¤©å¼€å­¦'), AIMessage(content='å¥½çš„ï¼Œæ˜å¤©è§')]
'''


```



### ConversationChains

```python
from langchain.chains import ConversationChain

# æ”¯æŒè®°å½•çš„å¯¹è¯chain
conversation = ConversationChain(llm=llm, verbose=True)
```



æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹template

```python
'''
'The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know. 
Current conversation: {history}
Human: {input}
AI:
'''
```



æ¯æ¬¡éƒ½ä¼šå­˜å‚¨ä¹‹å‰çš„å†å²è®°å½•ï¼Œæ”¾å…¥æœ¬æ¬¡çš„è¾“å…¥ï¼Œè®©AIå¡«å†™æ–°çš„å›å¤å†…å®¹

ä¸ºäº†é˜²æ­¢å›å¤è¯æ•°è¿‡å¤šï¼Œç¨å¾®ä¿®æ”¹äº†ä¸€ä¸‹ï¼š

```python
conversation.prompt.template = '''
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
Current conversation:
{history}
Human: {input}
AI:
æ³¨æ„ï¼Œåªå…è®¸ä½¿ç”¨ä¸è¶…è¿‡10ä¸ªå­—è¿›è¡Œç®€å•å›ç­”
'''

response = conversation.predict(input='æˆ‘å§å§æ˜å¤©è¦è¿‡ç”Ÿæ—¥ï¼Œæ¨èä¸€æŸç”Ÿæ—¥èŠ±æŸçš„ç§ç±»ã€‚')
response = conversation.predict(input='å¥¹å–œæ¬¢çš„é¢œè‰²æ˜¯ç²‰è‰²çš„ã€‚')
response = conversation.predict(input='æˆ‘ä¸ºä»€ä¹ˆè¦ä¹°èŠ±')


conversation.memory.chat_memory.messages
'''
[HumanMessage(content='æˆ‘å§å§æ˜å¤©è¦è¿‡ç”Ÿæ—¥ï¼Œæ¨èä¸€æŸç”Ÿæ—¥èŠ±æŸçš„ç§ç±»ã€‚'),
 AIMessage(content='"ç«ç‘°ã€ç™¾åˆã€éƒé‡‘é¦™ã€åº·ä¹ƒé¦¨ã€å¤ªé˜³èŠ±"ã€‚'),
 HumanMessage(content='å¥¹å–œæ¬¢çš„é¢œè‰²æ˜¯ç²‰è‰²çš„ã€‚'),
 AIMessage(content='ç²‰è‰²åº·ä¹ƒé¦¨ã€‚'),
 HumanMessage(content='æˆ‘ä¸ºä»€ä¹ˆè¦ä¹°èŠ±'),
 AIMessage(content='ä¸ºäº†åº†ç¥å¥¹ç”Ÿæ—¥ã€‚')]
'''
```



## Indexes

å¯¹æ–‡æ¡£ï¼ˆtxtã€pdfã€mdâ€¦â€¦ï¼‰è¿›è¡Œå¤„ç†ã€æ£€ç´¢

### TextLoader

```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader('source.txt')
txt = loader.load() # åŠ è½½æ–‡æ¡£

print(txt)
print(len(txt))

# [Document(metadata={'source': 'source.txt'}, page_content='ã€ä»£å·ã€‘ç¼ªå°”èµ›æ€\nã€æ€§åˆ«ã€‘å¥³\nã€æˆ˜æ–—ç»éªŒã€‘æ²¡æœ‰æˆ˜æ–—ç»éªŒ\nã€å‡ºèº«åœ°ã€‘æœªå…¬å¼€\nã€ç”Ÿæ—¥ã€‘11æœˆ3æ—¥\nã€ç§æ—ã€‘ç²¾çµ\nã€èº«é«˜ã€‘169cm\nã€çŸ¿çŸ³ç—…æ„ŸæŸ“æƒ…å†µã€‘\nå‚ç…§åŒ»å­¦æ£€æµ‹æŠ¥å‘Šï¼Œç¡®è®¤ä¸ºéæ„ŸæŸ“è€…ã€‚')]
# 1
```

### CharacterTextSplitter

-   å¸¸å°†æ–‡æœ¬åˆ‡åˆ†æˆå¤šä¸ªæ®µ
-   åˆ‡åˆ†æ–¹å¼
    -   æŒ‰å­—æ•°ï¼Œç®€å•ï¼Œä½†æ˜¯å®¹æ˜“åˆ‡å‡ºæ²¡æ„ä¹‰çš„æ®µè½
    -   æŒ‰ç‰¹æ®Šå­—ç¬¦ï¼Œä¾‹å¦‚å›è½¦ç¬¦

```python
from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(
    separator='\n', # åˆ‡åˆ†ç¬¦
    chunk_size=10,  # chunkå¤§å°
    chunk_overlap=0 # é‡å 
)

# åˆ‡å‰²æˆå­—ç¬¦ä¸²åˆ—è¡¨
txt_doc = text_splitter.split_text(txt[0].page_content)
print(txt_doc)

# åˆ‡å‰²æˆdocumentå¯¹è±¡åˆ—è¡¨
texts = text_splitter.split_documents(txt)
print(texts)

'''
['ã€ä»£å·ã€‘ç¼ªå°”èµ›æ€', 'ã€æ€§åˆ«ã€‘å¥³', 'ã€æˆ˜æ–—ç»éªŒã€‘æ²¡æœ‰æˆ˜æ–—ç»éªŒ', 'ã€å‡ºèº«åœ°ã€‘æœªå…¬å¼€', 'ã€ç”Ÿæ—¥ã€‘11æœˆ3æ—¥', 'ã€ç§æ—ã€‘ç²¾çµ', 'ã€èº«é«˜ã€‘169cm', 'ã€çŸ¿çŸ³ç—…æ„ŸæŸ“æƒ…å†µã€‘', 'å‚ç…§åŒ»å­¦æ£€æµ‹æŠ¥å‘Šï¼Œç¡®è®¤ä¸ºéæ„ŸæŸ“è€…ã€‚']

[Document(metadata={'source': 'source.txt'}, page_content='ã€ä»£å·ã€‘ç¼ªå°”èµ›æ€'), Document(metadata={'source': 'source.txt'}, page_content='ã€æ€§åˆ«ã€‘å¥³'), Document(metadata={'source': 'source.txt'}, page_content='ã€æˆ˜æ–—ç»éªŒã€‘æ²¡æœ‰æˆ˜æ–—ç»éªŒ'), Document(metadata={'source': 'source.txt'}, page_content='ã€å‡ºèº«åœ°ã€‘æœªå…¬å¼€'), Document(metadata={'source': 'source.txt'}, page_content='ã€ç”Ÿæ—¥ã€‘11æœˆ3æ—¥'), Document(metadata={'source': 'source.txt'}, page_content='ã€ç§æ—ã€‘ç²¾çµ'), Document(metadata={'source': 'source.txt'}, page_content='ã€èº«é«˜ã€‘169cm'), Document(metadata={'source': 'source.txt'}, page_content='ã€çŸ¿çŸ³ç—…æ„ŸæŸ“æƒ…å†µã€‘'), Document(metadata={'source': 'source.txt'}, page_content='å‚ç…§åŒ»å­¦æ£€æµ‹æŠ¥å‘Šï¼Œç¡®è®¤ä¸ºéæ„ŸæŸ“è€…ã€‚')]
'''
```



### å‘é‡æ•°æ®åº“

```python
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
import os

embeddings = DashScopeEmbeddings(
    model="text-embedding-v1", dashscope_api_key=os.environ["DASHSCOPE_API_KEY"]
)

db = FAISS.from_documents(documents, embeddings) # æ„å»ºæ•°æ®åº“
retriever = db.as_retriever( # è½¬æ¢ä¸ºæ£€ç´¢å™¨ è¿”å›æœ€ç›¸å…³çš„kä¸ªæ–‡æ¡£
    search_kwargs = {
        'k': 2
	}
)

result = retriever.get_relevant_documents("å­¦é™¢å…±æœ‰å‡ æ¬¡æ™‹çº§ICPCå›½é™…å¤§å­¦ç”Ÿç¨‹åºè®¾è®¡ç«èµ›å…¨çƒæ€»å†³èµ›ï¼Œåˆ†åˆ«æ˜¯å“ªå‡ å¹´ï¼Ÿ")
result

```

