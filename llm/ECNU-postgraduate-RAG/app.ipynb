{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf unstructured markdown faiss-cpu > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# 定义文件路径\n",
    "pdf_file = \"./data/2023级计算机科学与技术学术硕士研究生培养方案.pdf\"\n",
    "md_file1 = \"./data/历史沿革.md\"\n",
    "md_file2 = \"./data/学院简介.md\"\n",
    "\n",
    "# 加载文件内容\n",
    "pdf_loader = PyPDFLoader(file_path=pdf_file)\n",
    "md_loader1 = UnstructuredMarkdownLoader(file_path=md_file1)\n",
    "md_loader2 = UnstructuredMarkdownLoader(file_path=md_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "def split(chunk_size, doc):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    \tchunk_size = chunk_size,\n",
    "    \tchunk_overlap  = chunk_size // 10,\n",
    "\t)\n",
    "    return text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_documents = split(chunk_size=250, doc=pdf_loader.load())\n",
    "md_documents1 = split(chunk_size=50, doc=md_loader1.load())\n",
    "md_documents2 = split(chunk_size=100, doc=md_loader2.load())\n",
    "documents = pdf_documents + md_documents1 + md_documents2\n",
    "print(f'chunk nums = {len(documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "import os\n",
    "\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v1\", dashscope_api_key=os.environ[\"DASHSCOPE_API_KEY\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents, embeddings) # 构建数据库\n",
    "retriever = db.as_retriever( # 转换为检索器 返回最相关的k个文档\n",
    "    search_kwargs = {\n",
    "        'k': 10\n",
    "\t}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever.get_relevant_documents(\"2019年7月6日，华东师范大学成立了哪个学院？\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Tongyi\n",
    "# llm = Tongyi( model_name=\"qwen2-72b-instruct\", temperature=0.7, top_p=0.7 )\n",
    "# print( llm.invoke(\"你好吗？\") )\n",
    "\n",
    "from langchain_community.llms import Tongyi\n",
    "llm = Tongyi( model_name=\"qwen1.5-1.8b-chat\", temperature=0.95, top_p=0.7, max_tokens=10 )\n",
    "print( llm.invoke(\"你是一个基于华东师范大学计算机科学与技术学院知识库的问答助手，请打个招呼\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever,verbose=True)\n",
    "qa.combine_documents_chain.llm_chain.prompt.template = '''\n",
    "Use the following pieces of context to answer the question at the end by a sentence without any additional information. if you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '2019年7月6日，华东师范大学成立了哪个学院？'\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "examples = [\n",
    "    # {\n",
    "    #     'query':'学院在未来的发展目标是什么？',\n",
    "    #     'answer': '学院致力于建设成为世界一流的计算机科学与技术学院'\n",
    "\t# },\n",
    "    {\n",
    "        'query':'2019年7月6日，华东师范大学成立了哪个学院？',\n",
    "        'answer': '计算机科学与技术学院'\n",
    "\t},\n",
    "    # {\n",
    "    #     'query':'硕士研究生的总学分要求是多少？',\n",
    "    #     'answer': '硕士研究生的总学分要求是23学分'\n",
    "\t# }\n",
    "]\n",
    "\n",
    "predictions = qa.batch(inputs=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(eval_chain.prompt.template)\n",
    "# eval_chain.prompt.template = '''\n",
    "\n",
    "\n",
    "# Instructions:\n",
    "# You are a teacher grading a quiz.\n",
    "# You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either CORRECT or INCORRECT.\n",
    "\n",
    "# Example Format:\n",
    "# QUESTION: question here\n",
    "# STUDENT ANSWER: student's answer here\n",
    "# TRUE ANSWER: true answer here\n",
    "# GRADE: CORRECT or INCORRECT here\n",
    "\n",
    "# Guidelines:\n",
    "\n",
    "# - Ensure that the number of main entities in the student's answer matches the number of main entities in the question.\n",
    "# - Ignore minor differences in wording and punctuation.\n",
    "# - Ensure that there are no conflicting statements in the student's answer. If the answer contains conflicting information or incorrect entities, it should be marked INCORRECT.\n",
    "# - Additional information is acceptable only if it does not conflict with the true answer and does not introduce additional main entities.\n",
    "\n",
    "# QUESTION: {query}\n",
    "# STUDENT ANSWER: {result}\n",
    "# TRUE ANSWER: {answer}\n",
    "# GRADE:\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graded_outputs = eval_chain.evaluate(examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i][\"query\"])\n",
    "    print(\"Real Answer: \" + predictions[i][\"answer\"])\n",
    "    print(\"Predicted Answer: \" + predictions[i][\"result\"])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i][\"results\"])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(eval_chain.prompt.template)\n",
    "query = predictions[0]['query']\n",
    "answer = predictions[0]['answer']\n",
    "result = predictions[0]['result']\n",
    "\n",
    "prompt_txt =  prompt.format(query=query, answer=answer, result=result)\n",
    "print(prompt_txt)\n",
    "\n",
    "llm.invoke(prompt_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "# 初始化模型\n",
    "llm = Tongyi(\n",
    "    model_name=\"qwen1.5-1.8b-chat\",\n",
    "    temperature=0.95,\n",
    "    top_p=0.7\n",
    ")\n",
    "\n",
    "# 创建回调处理器实例\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# 使用模型进行预测，并启用流式输出\n",
    "response = llm(\"你的问题或者提示\", stream=True, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
