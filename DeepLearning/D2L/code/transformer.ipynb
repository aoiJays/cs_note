{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmaps(matrix):\n",
    "    matrix = matrix[0,0]\n",
    "    plt.figure(figsize=(8, 6)) \n",
    "    sns.heatmap(matrix,cmap=\"viridis\")\n",
    "    plt.title(\"Attention\")\n",
    "    plt.xlabel(\"Key\")\n",
    "    plt.ylabel(\"Query\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        # vocab 表示词汇表的大小，d_model 表示嵌入向量的维度\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.1111,  0.0000,  1.1111],\n",
       "         [ 0.9350,  0.6003,  0.0111,  1.1111],\n",
       "         [ 1.0103, -0.4624,  0.0222,  1.1109],\n",
       "         [ 0.1568, -1.1000,  0.0333,  1.1106],\n",
       "         [-0.8409, -0.7263,  0.0444,  1.1102],\n",
       "         [-1.0655,  0.3152,  0.0555,  0.0000],\n",
       "         [-0.3105,  1.0669,  0.0666,  1.1091],\n",
       "         [ 0.7300,  0.8377,  0.0777,  1.1084],\n",
       "         [ 1.0993, -0.1617,  0.0888,  1.1076],\n",
       "         [ 0.4579, -1.0124,  0.0999,  0.0000]]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    # 词嵌入维度 dropout概率 最大序列长度\n",
    "    def __init__(self, d_model, dropout, max_len = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # 对于序列的每个位置 初始化每一维\n",
    "        position_encoding = torch.zeros( max_len, d_model )\n",
    "        \n",
    "        # div_term : pos * exp( - (2i/d_model)ln1e4 ) \n",
    "            # 0~2i,2i+1~d_model-1\n",
    "            # -torch.arange(0, d_model, 2) / d_model   -> -2i/d_model\n",
    "        div_term = torch.exp( -torch.arange(0, d_model, 2) / d_model * math.log(10000.0) ) \n",
    "        position = torch.arange( max_len ).unsqueeze(1) # ->  [ [0], [1], [2], ... ]\n",
    "        \n",
    "        # position*div_term 广播机制 \n",
    "        # position : (max_len, 1) div_term : (1, d_model/2)\n",
    "        # -> (max_len, d_model/2)\n",
    "        half_pe = position*div_term\n",
    "        position_encoding[:, 0::2] = torch.sin(half_pe) # 每个token的偶数维度\n",
    "        position_encoding[:, 1::2] = torch.cos(half_pe) # 每个token的奇数维度\n",
    "        \n",
    "        # -> (1, max_len, d_model) 方便批量操作\n",
    "        position_encoding = position_encoding.unsqueeze(0)\n",
    "        \n",
    "        # 定义一组参数self.position_encoding \n",
    "        # 模型训练时不会更新 即调用 optimizer.step() 后该组参数不会变化，只可人为地改变它们的值\n",
    "        # 但是保存模型时，该组参数又作为模型参数不可或缺的一部分被保存\n",
    "        self.register_buffer('position_encoding', position_encoding)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, position , dim)\n",
    "        # 返回所有x.size(1)表示x的序列长度（可能含有pad）\n",
    "        # 广播机制：每个batch都进行同样操作\n",
    "        return self.dropout( x + self.position_encoding[:, : x.size(1) ] )\n",
    "    \n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "pe = PositionalEncoding(4, 0.1, 10)\n",
    "y = pe((torch.zeros(1, 10, 4)))\n",
    "# print(y.shape)\n",
    "\n",
    "# plt.plot(np.arange(1000), y[0, :, 3:7].data.numpy())\n",
    "# plt.legend([\"dim %d\"%p for p in [3,4,5,6]])\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def attention(query, key, value, mask=None, dropout=None):\n",
    "#     \"Compute 'Scaled Dot Product Attention'\"\n",
    "#     d_k = query.size(-1)\n",
    "#     scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "#     if mask is not None:\n",
    "#         scores = scores.masked_fill(mask == 0, -1e9)\n",
    "#     p_attn = F.softmax(scores, dim = -1)\n",
    "#     if dropout is not None:\n",
    "#         p_attn = dropout(p_attn)\n",
    "#     return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    # value,query,size : (batch_size, num_heads, seq_len, d_k) \n",
    "    # num_heads会用到多头注意力机制中\n",
    "    \n",
    "    d_k = query.size(-1) # 最后一维 表示向量维度大小\n",
    "    \n",
    "    # 计算相似分数\n",
    "    # key交换最后两个维度 实质上是进行转置\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    if mask is not None:\n",
    "        # pytorch自带函数 满足第一个条件mask == 0时 赋值为：-1e9\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # 按最后一个维度去softmax\n",
    "    # score:(batch_size, num_heads, seq_len, seq_len) \n",
    "    # 每个token对其他所有token的分数\n",
    "    attention_weights = F.softmax(scores, dim=-1) \n",
    "    \n",
    "    if dropout is not None:\n",
    "        # 引入一点噪声\n",
    "        attention_weights = dropout(attention_weights)\n",
    "        \n",
    "    return torch.matmul(attention_weights, value), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3, 3])\n",
      "torch.Size([2, 1, 3, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 创建一些测试数据\n",
    "# 这里假设每个输入的维度是 d_k = 4，序列长度为 3\n",
    "batch_size = 2\n",
    "num_heads = 1\n",
    "seq_len = 3\n",
    "d_k = 4\n",
    "\n",
    "# 生成随机的 query, key, value 张量\n",
    "query = torch.rand(batch_size, num_heads, seq_len, d_k)\n",
    "key = torch.rand(batch_size, num_heads, seq_len, d_k)\n",
    "value = torch.rand(batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "# 生成可选的 mask 张量\n",
    "mask = torch.ones(batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "# 测试 attention 函数\n",
    "output, attention_weights = attention(query, key, value, mask)\n",
    "\n",
    "# 输出测试结果\n",
    "print(attention_weights.shape)\n",
    "print(output.shape)\n",
    "\n",
    "show_heatmaps(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clones(module, N):\n",
    "#     return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "# # 假设 MultiHeadedAttention 类已经定义\n",
    "# class MultiHeadedAttention(nn.Module):\n",
    "#     def __init__(self, h, d_model, dropout=0.1):\n",
    "#         super(MultiHeadedAttention, self).__init__()\n",
    "#         assert d_model % h == 0\n",
    "        \n",
    "#         self.d_k = d_model // h\n",
    "#         self.h = h\n",
    "#         self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "#         self.attn = None\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "#     def forward(self, query, key, value, mask=None):\n",
    "#         if mask is not None:\n",
    "#             mask = mask.unsqueeze(1)\n",
    "\n",
    "#         nbatches = query.size(0)\n",
    "        \n",
    "#         # 切分成多头\n",
    "#         ## value,query,size : (batch_size, seq_len, d_model) \n",
    "#         ## l(x): (batch_size, seq_len, d_model) \n",
    "#         ## .view(nbatches, -1, self.h, self.d_k) -> (batch_size, seq_len, num_heads, d_k)\n",
    "#         ## .transpose(1, 2) -> (batch_size, num_heads, seq_len, d_k) 转置操作\n",
    "        \n",
    "#         # zip -> (linear0, query) (linear1, key) (linear2, value)\n",
    "#         # 最后一个linear没有被绑定\n",
    "        \n",
    "#         query, key, value = [\n",
    "#             l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "#             for l, x in zip(self.linears, (query, key, value))\n",
    "#         ]\n",
    "        \n",
    "#         # 求解注意力\n",
    "#         x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        \n",
    "#         # 此时x:(batch_size, num_heads, seq_len, d_model)\n",
    "#         # 转置回来 重新变成(batch_size, seq_len, num_heads, d_k)\n",
    "#         # 修正存储 保证数值一定是连续存储的 .contiguous()\n",
    "#         # 否则无法使用.view() -> (batch_size, seq_len , d_model)\n",
    "#         x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)        \n",
    "        \n",
    "#         # 使用最后一个linear\n",
    "#         return self.linears[-1](x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, n):\n",
    "    # 深拷贝\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(n)])\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.h, self.d_k = h, d_model//h\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = None\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 切分成多头\n",
    "        ## value,query,size : (batch_size, seq_len, d_model) \n",
    "        ## linear(x): (batch_size, seq_len, d_model) \n",
    "        ## .view(batche_size, -1, self.h, self.d_k) -> (batch_size, seq_len, num_heads, d_k)\n",
    "        ## .transpose(1, 2) -> (batch_size, num_heads, seq_len, d_k) 转置操作\n",
    "        \n",
    "        # zip -> (linear0, query) (linear1, key) (linear2, value)\n",
    "        # 最后一个linear没有被绑定\n",
    "        \n",
    "        query, key, value = [\n",
    "            linear(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for linear, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        \n",
    "        # 此时x:(batch_size, num_heads, seq_len, d_model)\n",
    "        # 转置回来 重新变成(batch_size, seq_len, num_heads, d_k)\n",
    "        # 修正存储 保证数值一定是连续存储的 .contiguous()\n",
    "        # 否则无法使用.view() -> (batch_size, seq_len , d_model)\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_k*self.h)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 5, 16])\n",
      "Test passed with mask!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 测试 MultiHeadedAttention\n",
    "def test_multi_headed_attention_with_mask():\n",
    "    # 定义一些参数\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    h = 4\n",
    "    \n",
    "    # 随机生成一些输入张量\n",
    "    query = torch.rand(batch_size, seq_len, d_model)\n",
    "    key = torch.rand(batch_size, seq_len, d_model)\n",
    "    value = torch.rand(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 创建一个 mask，假设要忽略序列中的最后一个时间步\n",
    "    mask = torch.ones(batch_size, num_heads, seq_len)\n",
    "    mask[:, -1] = 0  # 忽略最后一个时间步\n",
    "\n",
    "    # 创建 MultiHeadedAttention 实例\n",
    "    mha = MultiHeadedAttention(h, d_model)\n",
    "    \n",
    "    # 前向传播\n",
    "    output = mha(query, key, value, mask=mask)\n",
    "    \n",
    "    # 打印输出的形状\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    \n",
    "    # 检查输出的维度是否符合预期\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \"Output shape mismatch!\"\n",
    "    \n",
    "    print(\"Test passed with mask!\")\n",
    "\n",
    "# 运行测试\n",
    "test_multi_headed_attention_with_mask()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1000, 512])\n"
     ]
    }
   ],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model) # Dropout不能放在最后\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.ffn(x)\n",
    "    \n",
    "temp = PositionwiseFeedForward(512, 2048)\n",
    "x = torch.rand(5,1000, 512)\n",
    "x = temp(x)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a = nn.Parameter(torch.ones(dim))\n",
    "        self.b = nn.Parameter(torch.zeros(dim))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True) # 求出最后一维的均值\n",
    "        std = x.std(dim=-1, keepdim=True) # 求出最后一维的方差\n",
    "        \n",
    "        # keepdim 把最后一个维度的所有数据替换成一个数字：计算结果\n",
    "        # 其他维度不变，方便后续的广播\n",
    "        \n",
    "        # 防止除以0\n",
    "        return self.a * (x - mean) / (std + self.eps) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([5, 1000, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "layer_norm = LayerNorm(d_model)\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "output = layer_norm( torch.randn(batch_size, seq_len, d_model) )\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, dim, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer( self.norm(x) ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, self_attention, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = self_attention # encoder使用的自注意力机制\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones( SublayerConnection(d_model, dropout), 2) # encoder两次残差归一化层\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # 通过自注意力机制\n",
    "        # sublayer.forward传入(x,func)两个参数 第二个参数要求是函数\n",
    "        # 使用lambda封装 \n",
    "        # x同时作为query key value\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attention(x,x,x,mask) )\n",
    "        \n",
    "        # 通过Feed-Forward\n",
    "        return self.sublayer[-1](x, self.feed_forward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N) # layer层重复N次\n",
    "        self.norm = LayerNorm(layer.d_model)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # 滚动N遍encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.d_model = d_model \n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(d_model, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory # encoder的输出\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) # 结合encoder\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.d_model)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 对数 softmax 以防止数值不稳定性\n",
    "        # 返回概率分布\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed \n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "   \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "         \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3730/2280125763.py:16: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model\n",
    "tmp_model = make_model(10, 10, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
