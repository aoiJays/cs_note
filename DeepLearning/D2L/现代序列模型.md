# 序列模型

>   **动手学深度学习v2** - https://zh-v2.d2l.ai/
>
>   
>
>   个人评价是需要有一点基础
>
>   -   [Pytorch 小土堆](https://www.bilibili.com/video/BV1hE411t7RN) 先把Pytorch基础看一下
>   -   [李宏毅2022春机器学习](https://www.bilibili.com/video/BV1Wv411h7kN)
>       -   理论部分更推荐李宏毅或者吴恩达，会更好理解
>       -   我的策略是理论在李宏毅这里补，作业不做，在李沐这里实操一下代码
>
>   本文不会放太多理论的东西
>
>   记录一下代码实操即可
>
>   理论请移步李宏毅课程的相关笔记

[TOC]




## 门控循环单元GRU 

并不是每个细节都值得关注

随着喂入序列的变长，序列开头的影响占比会变小

但很有可能序列开头存在重要的关键词

因此我们希望我们的网络能够对序列的不同部分，有所侧重、关注、选择



> - 存储：早期重要信息
> - 跳过：无用信息（网页文章的html代码）
> - 重置：书的章节之间的逻辑中断

### 门

![image-20240815015651031](./现代序列模型.assets/image-20240815015651031.png)
$$
R_t = \sigma( X_tW_{xr} +H_{t-1}W_{hr} + b_r ) \\
Z_t = \sigma( X_tW_{xz} +H_{t-1}W_{hz} + b_z )
$$

- 利用sigmoid函数，全连接层通过输入、隐状态，预测出门的值（0-1之间）

### 候选隐状态 <- 重置门

正常情况下，隐状态的计算：
$$
H_t =\phi(X_tW_{xh} + H_{t-1}W_{hh} + b_h)
$$
我们希望引入$R_t$，对状态进行**重置**：

- 设定激活函数为$\tanh$，确保候选隐状态值在(-1,1)内
- 使用Hadamard积（矩阵元素对应相乘）：$R_t \odot H_{t-1}$
    - $R_t \to 0$，此时$H_t$只由当前输入决定。相当于重置了隐状态为一开始的默认值，从头开始
    - $R_t \to 1$​，正常的循环神经网络，隐状态照常保存

综上，定义候选隐状态为：
$$
\tilde{H_t} =\tanh(X_tW_{xh} + (R_t \odot H_{t-1})W_{hh} + b_h)
$$
![image-20240815021155828](./现代序列模型.assets/image-20240815021155828.png)

### 隐状态 <- 更新门

上文中，我们计算得到的是**候选隐状态**

但是如果当前的文本并不让我们感到有意义，我们需要跳过这部分

也就是基本不会修改，直接沿用之前的隐状态

反之，我们希望将当前值更新为最新的隐状态

引入更新门：
$$
H_t = Z_t\odot H_{t- 1} + (1-Z_t)\odot \tilde{H_t}
$$

- $Z_t \to 0$，相当于完全使用当前新的隐状态
- $Z_t \to 1$，直接沿用之前的隐状态

![image-20240815021600369](./现代序列模型.assets/image-20240815021600369.png)



总结：

- 重置门：有助于捕获序列的**短期**依赖关系
- 更新门：有助于捕获序列的**长期**依赖关系

### 代码

`RNN`换成`GRU`即可

```python
class RNNModel(nn.Module):
    def __init__(self, vocab, **kwargs):
        
        super(RNNModel, self).__init__(**kwargs)
        
        self.vocab_size = len(vocab)
        self.num_hiddens = 256
    
        self.rnn = nn.GRU( ##### 唯一区别 ############
            input_size = self.vocab_size, # 输入数据的维度（28种字符）
            hidden_size = self.num_hiddens,  # 隐藏层维度
            num_layers = 1  # 隐藏层层数
        )
        
        # 输出层 由rnn的隐藏层 预测-> 每一个vocab的概率
        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
```

![image-20240815023150171](./现代序列模型.assets/image-20240815023150171.png)

- 相比RNN，困惑度整体会变更低

