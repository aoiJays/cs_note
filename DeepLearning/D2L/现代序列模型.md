# 序列模型

>   **动手学深度学习v2** - https://zh-v2.d2l.ai/
>
>   
>
>   个人评价是需要有一点基础
>
>   -   [Pytorch 小土堆](https://www.bilibili.com/video/BV1hE411t7RN) 先把Pytorch基础看一下
>   -   [李宏毅2022春机器学习](https://www.bilibili.com/video/BV1Wv411h7kN)
>       -   理论部分更推荐李宏毅或者吴恩达，会更好理解
>       -   我的策略是理论在李宏毅这里补，作业不做，在李沐这里实操一下代码
>
>   本文不会放太多理论的东西
>
>   记录一下代码实操即可
>
>   理论请移步李宏毅课程的相关笔记

[TOC]




## 门控循环单元GRU 

并不是每个细节都值得关注

随着喂入序列的变长，序列开头的影响占比会变小

但很有可能序列开头存在重要的关键词

因此我们希望我们的网络能够对序列的不同部分，有所侧重、关注、选择



> - 存储：早期重要信息
> - 跳过：无用信息（网页文章的html代码）
> - 重置：书的章节之间的逻辑中断

### 门

![image-20240815015651031](./现代序列模型.assets/image-20240815015651031.png)
$$
R_t = \sigma( X_tW_{xr} +H_{t-1}W_{hr} + b_r ) \\
Z_t = \sigma( X_tW_{xz} +H_{t-1}W_{hz} + b_z )
$$

- 利用sigmoid函数，全连接层通过输入、隐状态，预测出门的值（0-1之间）

### 候选隐状态 <- 重置门

正常情况下，隐状态的计算：
$$
H_t =\phi(X_tW_{xh} + H_{t-1}W_{hh} + b_h)
$$
我们希望引入$R_t$，对状态进行**重置**：

- 设定激活函数为$\tanh$，确保候选隐状态值在(-1,1)内
- 使用Hadamard积（矩阵元素对应相乘）：$R_t \odot H_{t-1}$
    - $R_t \to 0$，此时$H_t$只由当前输入决定。相当于重置了隐状态为一开始的默认值，从头开始
    - $R_t \to 1$​，正常的循环神经网络，隐状态照常保存

综上，定义候选隐状态为：
$$
\tilde{H_t} =\tanh(X_tW_{xh} + (R_t \odot H_{t-1})W_{hh} + b_h)
$$
![image-20240815021155828](./现代序列模型.assets/image-20240815021155828.png)

### 隐状态 <- 更新门

上文中，我们计算得到的是**候选隐状态**

但是如果当前的文本并不让我们感到有意义，我们需要跳过这部分

也就是基本不会修改，直接沿用之前的隐状态

反之，我们希望将当前值更新为最新的隐状态

引入更新门：
$$
H_t = Z_t\odot H_{t- 1} + (1-Z_t)\odot \tilde{H_t}
$$

- $Z_t \to 0$，相当于完全使用当前新的隐状态
- $Z_t \to 1$，直接沿用之前的隐状态

![image-20240815021600369](./现代序列模型.assets/image-20240815021600369.png)



总结：

- 重置门：有助于捕获序列的**短期**依赖关系
- 更新门：有助于捕获序列的**长期**依赖关系

### 代码

`RNN`换成`GRU`即可

```python
class RNNModel(nn.Module):
    def __init__(self, vocab, **kwargs):
        
        super(RNNModel, self).__init__(**kwargs)
        
        self.vocab_size = len(vocab)
        self.num_hiddens = 256
    
        self.rnn = nn.GRU( ##### 唯一区别 ############
            input_size = self.vocab_size, # 输入数据的维度（28种字符）
            hidden_size = self.num_hiddens,  # 隐藏层维度
            num_layers = 1  # 隐藏层层数
        )
        
        # 输出层 由rnn的隐藏层 预测-> 每一个vocab的概率
        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
```

![image-20240815023150171](./现代序列模型.assets/image-20240815023150171.png)

- 相比RNN，困惑度整体会变更低



## 长短期记忆网络LSTM

>   设计上比GRU更加复杂，但是早了20年

### 门

![image-20240815200752066](./现代序列模型.assets/image-20240815200752066.png)

-   输入门：

$$
I_t = \sigma(X_tW_{xi} + H_{t_1}W_{hi} + b_i)
$$

-   遗忘门：
$$
F_t = \sigma(X_tW_{xf} + H_{t_1}W_{hf} + b_f)
$$

- 输出门：

$$
O_t = \sigma(X_tW_{xo} + H_{t_1}W_{ho} + b_o)
$$

>   通过sigmoid激活函数，三个门的值都在(0,1)内

### 候选记忆元

使用$\tanh$做激活函数，取值`[-1,1]`

![image-20240815201321767](./现代序列模型.assets/image-20240815201321767.png)
$$
\tilde{C_t} = \tanh(X_tW_{xc}+H_{t-1}W_{hc}+b_c)
$$

### 记忆元

记忆元主要来自两个部分：

-   过去的记忆：即$C_{t-1}$，由遗忘门$F_t$控制保留多少过去的记忆
-   新的记忆：即当前输入带来的候选记忆元$\tilde{C_t}$，由输入门控制引入多少

则：
$$
C_t = F_t\odot C_{t-1}+I_t\odot \tilde{C_t}
$$
![image-20240815202050099](./现代序列模型.assets/image-20240815202050099.png)

>   这种机制有助于模型记录下非常久远以前的记忆
>
>   某种程度上缓解了梯度消失，捕获长距离依赖关系



### 隐状态

$$
H_t = O_t\odot\tanh(C_t)
$$

-   确保隐状态仍然在$[-1,1]$
-   输出门接近1：完整保留记忆作为隐状态
-   输出门接近0：隐状态被重置，只保留了记忆信息

![image-20240815202726087](./现代序列模型.assets/image-20240815202726087.png)



## 深度RNN

前一篇讲过了（

其实就是隐状态由单个全连接层，变成多个隐藏层

![image-20240815203100752](./现代序列模型.assets/image-20240815203100752.png)

## 双向RNN

普通的RNN只能考虑到上文，无法考虑到下文：

```python
'''
我___。

我___饿了。

我___饿了，我可以吃半头猪。
'''
```

![image-20240815205000491](./现代序列模型.assets/image-20240815205000491.png)

将隐状态分为正向隐状态、反向隐状态

对于正向隐状态：

![image-20240815205044213](./现代序列模型.assets/image-20240815205044213.png)
$$
\overrightarrow{H_t}=  \phi( X_tW_{xh}^{\text{front}} + \overrightarrow{H}_{t-1}W_{hh}^{\text{front}}+b_h^{\text{front}})
$$

-   由输入、上一个前向隐状态得到

对于反向隐状态：

![image-20240815205315871](./现代序列模型.assets/image-20240815205315871.png)
$$
\overleftarrow{H_t}=  \phi( X_tW_{xh}^{\text{back}} + \overleftarrow{H}_{t+1}W_{hh}^{\text{back}}+b_h^{\text{back}})
$$

-   由输入、后一个反向隐状态得到



对于输出，我们把$\overrightarrow{H_t},\overleftarrow{H_t}$合并成$H_t$（矩阵连起来）
$$
O_t = H_TW_{hq} + b_q
$$
![image-20240815205543222](./现代序列模型.assets/image-20240815205543222.png)



#### 代价

-   计算速度非常慢，计算链条很长
-   需要存储的内存非常大

-   用处有限
    -   填充缺失单词、词元、注释
    -   机器翻译



## 机器翻译 



