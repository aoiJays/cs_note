# L6. Generation

>   [ML 2022 Spring (ntu.edu.tw)](https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php)
>
>   https://www.bilibili.com/video/BV1VN4y1P7Zj

[TOC]




## Generative Adversarial Network(GAN)

GAN分成两个部分：

- 生成模型：Generative Model
- 判别模型：Discriminative Model

我们一开始拥有分别拥有这两个初始化的模型

以图片生成为例进行训练，我们需要采集大量图片

![image-20240902004404415](./L6.assets/image-20240902004404415.png)

- 固定生成模型，并且生成大量图片，对判别模型进行分类训练
    - Database中的图片标签为1
    - 生成的图片标签为0
    - 我们希望判别模型能够判断图片是否生成或真实
- 固定判别模型，对生成模型进行训练，喂入判别模型，希望输出尽可能大（更加接近真实图片）

![image-20240902004841244](./L6.assets/image-20240902004841244.png)



交替两个步骤重复多次即可

- 生成模型经过多次迭代，产生的图片能够尽可能混淆判别模型，也就会更加接近真实图片
- 判别模型由于生成模型变得更加真实，为了进行正确分类，对分类的要求也会更加苛刻

### Distribution

- 特别是需要**创造力**的任务
- 同样的输入可以有多种输出

> Drawing、Chat bot
>
> 模型为了讨好损失函数，对于多种输出会既要又要
>
> 例如视频生成，比如一辆车有左转和右转的可能，可能就会分裂成两辆车，一个左转一个右转
>
> 因此我们需要引入概率分布，对每种选择都以概率形式进行表达

Generator如何生成不同的图片或其他结果

我们可以给定一个输入向量，向量经过网络结构后则会得到一个输出

这个输入向量来源于一个用于输入的Distribution

比如正态分布……

从输入Distribution中sample多个向量作为输入，就可以产生大量生成

### Objective

接下来我们从Distribution的角度对GAN进行解释

![image-20240902005824500](./L6.assets/image-20240902005824500.png)

假设我们的输入服从正态分布

输入经过Generator后产生的输出服从$P_G$

而我们的真实数据集的分布为$P_{data}$

我们的目标就是让这两个尽可能接近

但是我们很难通过函数的方式计算两个概率分布的距离，从而设计损失函数进行训练

> Divergence（散度）：可以用来表示，但是需要积分运算，对于应用来说不现实

- 从$P_{data}$中sample
- 从正态分布中sample（也相当于$P_G$中sample）

用Discriminator进行分类器训练

（损失函数不是交叉熵函数）

设计了一个特殊函数，和交叉熵函数比较相似，实质上与Divergence是相关的

- Divergence越大，说明越容易分类
- Divergence越小，说明越困难分类



### Wasserstein Distance

GAN的训练比较困难

我们生成符合要求的图片，其$P_{data}$实质上是高维向量空间中一段非常狭窄的区域（假设为直线）

因此大部分时候$P_G$和$P_{data}$根本不相交

- 从Divergence的角度，只要是不相交，Divergence都是一个值

![image-20240902011924402](./L6.assets/image-20240902011924402.png)

- 从二分类的角度来说，由于图片差距过大，我们非常容易得到100%的分类正确结果，因此损失函数值的大小完全没有意义（相关的Divergence是常数）

> 一般的梯度下降问题只要看Loss下降就知道是work的
>
> 但是这个问题使得我们难以通过Loss知道是否在正确训练

引入了Wasserstein Distance

![image-20240902013914086](./L6.assets/image-20240902013914086.png)

> 把P通过搬运每个点，使其变成Q的最小移动距离

![image-20240902014012803](./L6.assets/image-20240902014012803.png)

这样我们就可以在不相交的情况下衡量优劣

具体的实现方式需要修改损失函数

**并且要求Discriminator足够平滑**

![image-20240902014303048](./L6.assets/image-20240902014303048.png)

如果不够平滑，分类器Discriminator对毫不相交的两类进行分类时

会希望两类差距尽可能大

因此generated的函数值会变成负无穷，real的函数值会变成正无穷

那么两个无穷的差值仍然是无穷，我们依旧无法衡量两者的差距

不能解决Divergence的问题





但如果足够平滑（变化不会很剧烈）

- 距离近：函数值有界，两个值差距不会太大，减一下就是差值
- 距离远：减一下就是差值
- 距离特别远：那么无穷减去无穷也没什么问题，因为差距确实很大



#### WGAN

就是使用Wasserstein Distance的GAN

那么要如何使得Discriminator平滑

- WGAN的近似方法：对参数裁剪，绝对值超过c就赋值为c，压缩函数空间
- Improved WGAN的方法：Gradient Penalty，没听懂
- SNGAN最终方法：Spectral Normalization，让梯度的标准差在任意处小于1 

 

### GAN for Sequence

**我们发现很难训练出一个产生sequence的GAN**

我们使用某种架构产生一个关于多个token的distribution

Discriminator则会根据概率最大的token作为输入进行判别打分

但是当我们进行训练时，对Generator进行一点调整，很有可能原本的token仍然是概率最大的

因此不会改变Discriminator的打分输出，微分为0，很难进行梯度下降



### Evaluation





 
